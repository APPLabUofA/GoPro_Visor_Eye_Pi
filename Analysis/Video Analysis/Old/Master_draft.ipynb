{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References\n",
    "\n",
    "https://www.pyimagesearch.com/2014/08/04/opencv-python-color-detection/ \n",
    "\n",
    "https://www.geeksforgeeks.org/detection-specific-colorblue-using-opencv-python/ (basic colour mask)\n",
    "\n",
    "https://www.learnopencv.com/color-spaces-in-opencv-cpp-python/ (colour space)\n",
    "\n",
    "https://www.sparkfun.com/news/2191?__hstc=77938635.35c34ab7bf88e972fdd7a7debc8575ba.1474848000139.1474848000140.1474848000141.1&__hssc=77938635.1.1474848000142&__hsfp=1773666937 \n",
    "\n",
    "https://docs.opencv.org/2.4/modules/imgproc/doc/histograms.html?highlight=comparehist#comparehist (SetHistBinRanges, ThreshHist, ReleaseHist, ClearHist)\n",
    "\n",
    "https://docs.opencv.org/2.4/doc/tutorials/imgproc/histograms/histogram_comparison/histogram_comparison.html \n",
    "\n",
    "https://stackoverflow.com/questions/32414559/opencv-contour-minimum-dimension-location-in-python \n",
    "\n",
    "https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_imgproc/py_contours/py_contour_features/py_contour_features.html (contour features)\n",
    "\n",
    "https://docs.opencv.org/2.4/doc/tutorials/imgproc/histograms/histogram_calculation/histogram_calculation.html#code \n",
    "\n",
    "https://github.com/abidrahmank/OpenCV2-Python/blob/master/Official_Tutorial_Python_Codes/3_imgproc/comparehist.py (custom histogram comparisons)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from pycharm IDE - running python 3.6 - some expected conversion errors\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import time\n",
    "#import ffmpeg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If needed, convert MP4 to avi directly first, might be best to run directly from command window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "ffmpeg_file path = \\\\Users\\\\User\\\\ffmpeg-4.1-win64-static\\\\bin\\\\ffmpeg.exe ## example path to ffmpeg if path variable is not set or working\n",
    "Original = M:\\Data\\GoPro_Visor\\GoPro_Video\\003_setup.MP4 # example mp4\n",
    "Output = M:\\Data\\GoPro_Visor\\Converted_Video\\003_setup.avi # example output .avi format\n",
    "\n",
    "ffmpeg_file path -i Original Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define video manipulation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram equalization parameters\n",
    "\n",
    "def split_into_rgb_channels(image):\n",
    "  '''Split the target image into its red, green and blue channels.\n",
    "  image - a numpy array of shape (rows, columns, 3).\n",
    "  output - three numpy arrays of shape (rows, columns) and dtype same as\n",
    "           image, containing the corresponding channels. \n",
    "  '''\n",
    "  red = image[:,:,2]\n",
    "  green = image[:,:,1]\n",
    "  blue = image[:,:,0]\n",
    "  return red, green, blue\n",
    "\n",
    "def equalizeHistColor(frame):\n",
    "    # equalize the histogram of color image\n",
    "    img = cv2.cvtColor(frame, cv2.COLOR_RGB2HSV)  # convert to HSV\n",
    "    img[:, :, 2] = cv2.equalizeHist(img[:, :, 2])  # equalize the histogram of the V channel\n",
    "    return cv2.cvtColor(img, cv2.COLOR_HSV2RGB)  # convert the HSV image back to RGB format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What file are we inputting?\n",
    "# Specify file pathway unless in the GoPro_Grid_Pi directory\n",
    "webcam = 0 # set to 1 if input is a webcam\n",
    "\n",
    "part = '003' # Version - example '001' or '054'\n",
    "exp = '_camera_p3' # ex. '003_camera_p3'\n",
    "in_format = '.avi'\n",
    "in_file = part + exp + in_format\n",
    "\n",
    "if webcam == 1:\n",
    "    in_file = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are we saving an output file (file with overlaid filters/bounders/manipulations)?\n",
    "\n",
    " # # Version - example '001' or '054'\n",
    "out_format = '.avi'\n",
    "out_file = part + exp + out_format\n",
    "\n",
    "# Output file parameter\n",
    "imgSize=(640,480) # likely best to set to original  dimensions\n",
    "frame_per_second=30.0\n",
    "writer = cv2.VideoWriter(out_format, cv2.VideoWriter_fourcc(*\"MJPG\"), frame_per_second,imgSize,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize event times of the appropriate participant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for 003 (video 0079)%%%\n",
    "start_eeg = 652\n",
    "door_closed = 4800\n",
    "start_flash = 8897"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#for 004 (video 0080)%%%\n",
    "start_eeg = 3041\n",
    "door_closed = 6947\n",
    "start_flash = 12159\n",
    "\n",
    "#for 005 (video 0081)%%%\n",
    "start_eeg = 3330\n",
    "door_closed = 12240\n",
    "start_flash = 3268+14400\n",
    "\n",
    "#for 006 (video 0082?)%%%\n",
    "start_eeg = 567\n",
    "door_closed = 5040\n",
    "start_flash = 10446\n",
    "\n",
    "#for 007 (video 0083?)%%%\n",
    "start_eeg = 1045\n",
    "door_closed = 7440\n",
    "start_flash = 12567\n",
    "#off_flash =\n",
    "\n",
    "#for 008 (video 0084?)%%%\n",
    "start_eeg = 2053\n",
    "door_closed = 7680\n",
    "start_flash = 12673\n",
    "\n",
    "#for 009 (video 0084?)%%%\n",
    "start_eeg = 616\n",
    "door_closed = 6000\n",
    "start_flash = 11040\n",
    "\n",
    "#for 010 (video 0087)%%%\n",
    "start_eeg = 1443\n",
    "door_closed = 8640\n",
    "start_flash = 13343\n",
    "\n",
    "#for 011 (video 0088)%%%\n",
    "start_eeg = 638\n",
    "door_closed = 8400\n",
    "start_flash = 13176"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initalize doubles for start/end of flash trains + state list for each frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "# First Pass\n",
    "# List of [frame + start event trigger] (where the max[index] = corresponds with the last EEG event)\n",
    "Trigger_Start = []\n",
    "# List of [frame + end event trigger]\n",
    "Trigger_Stop = []\n",
    "# List of [frame + trigger state] (0 B + G channels below thresholds, 1 above B channel threshold, 2 above R channel threshold\n",
    "Tigger_State = []\n",
    "\n",
    "# Want a frame of each start trigger saved to a folder\n",
    "\n",
    "# Second Pass for extracting epochs based off first pass - figure out later\n",
    "# Eventually will output an ~[-1,1] video epoch to be the raw input for deep learning\n",
    "Trigger_Epoch = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## frame dimensions (x,y) & img dimensions (2x,2y) # for resizing drawn frames\n",
    "scaling_factorx=1 \n",
    "scaling_factory=1 \n",
    "scaling_factor2x=1 \n",
    "scaling_factor2y=1\n",
    "\n",
    "# Manipulation Variables,\n",
    "kernelSize = 21\n",
    "GB_Kernel = 21\n",
    "\n",
    "# Edge Detection Parameter\n",
    "parameter1=20\n",
    "parameter2=60\n",
    "\n",
    "intApertureSize=1\n",
    "# Colours\n",
    "custom_color_list = []\n",
    "custom_color_list = \\\n",
    "    [\"COLOR_BGR2RGB\",\n",
    "     \"COLOR_RGB2BGR\",\n",
    "     \"COLOR_BGR2GRAY\",\n",
    "     \"COLOR_RGB2GRAY\",\n",
    "     \"COLOR_BGR2HSV\",\n",
    "     \"COLOR_RGB2HSV\",\n",
    "     \"COLOR_RGB2HLS\",\n",
    "     \"COLOR_BGR2HLS\",\n",
    "     \"COLOR_BGR2XYZ\",\n",
    "     \"COLOR_RGB2XYZ\",\n",
    "     \"COLOR_BGR2Lab\",\n",
    "     \"COLOR_RGB2Luv\"]\n",
    "\n",
    "custom_color_type = 1 # 1-12 - look at elements in custom_color_list\n",
    "\n",
    "# Thresholding \n",
    "threshold1=100\n",
    "threshold2=200\n",
    "\n",
    "# Contours\n",
    "color=(255,0,0)\n",
    "thickness=2\n",
    "count = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run through each frame from door closed indexed from start_eeg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "green hsv = [[[ 60 255 255]]]\n"
     ]
    }
   ],
   "source": [
    "# Find a given colours hsv value to threshold around\n",
    "\n",
    "green = np.uint8([[[0,255,0 ]]])\n",
    "hsv_green = cv2.cvtColor(green,cv2.COLOR_BGR2HSV)\n",
    "print('green hsv = {}'.format(hsv_green))\n",
    "#[[[ 60 255 255]]]\n",
    "\n",
    "# red = np.uint8([[[255,0,0 ]]])\n",
    "# hsv_red = cv2.cvtColor(red,cv2.COLOR_BGR2HSV)\n",
    "# print('red hsv = ' + hsv_red)\n",
    "\n",
    "# blue = np.uint8([[[255,0,0 ]]])\n",
    "# hsv_blue = cv2.cvtColor(blue,cv2.COLOR_BGR2HSV)\n",
    "# print('blue hsv = ' + hsv_blue)\n",
    "      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the colour boundaries + size of detector + buffer of frames around event if outputting epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# define the lower and upper boundaries of the \"green\" ball in\n",
    "# the HSV color space\n",
    "# redLower = ()\n",
    "# redUpper = ()\n",
    "greenLower = (29, 86, 6)\n",
    "greenUpper = (64, 255, 255)\n",
    "blueLower = (86, 31, 4)\n",
    "blueUpper = (220, 88, 50)\n",
    "colours = [red, green, blue]\n",
    "for i in len(colours)\n",
    "# bgr = [40, 158, 16]\n",
    "# thresh = 20\n",
    "# minBGR = np.array([bgr[0] - thresh, bgr[1] - thresh, bgr[2] - thresh])\n",
    "# maxBGR = np.array([bgr[0] + thresh, bgr[1] + thresh, bgr[2] + thresh])  \n",
    "# maskBGR = cv2.inRange(bright,minBGR,maxBGR)\n",
    "# resultBGR = cv2.bitwise_and(bright, bright, mask = maskBGR)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cam' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-a61fa18305fc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#get the frame number\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcam\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCAP_PROP_POS_FRAMES\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cam' is not defined"
     ]
    }
   ],
   "source": [
    "#get the frame number\n",
    "#start = cam.get(cv2.CAP_PROP_POS_FRAMES)\n",
    "#print(start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(in_file)  # load the video\n",
    "cap.set(2,frame_no) # first arguement of 2 indicated a zero-based index - and second indicates the frame_number that is the focus\n",
    "counter = 0 # which frame\n",
    "while (cap.isOpened()):  # play the video by reading frame by frame\n",
    "    ret, frame = cap.read()\n",
    "    if ret == True:\n",
    "        split_into_rgb_channels(frame) # seperate into 3 seperate RGB frames\n",
    "        \n",
    "        # equalize the histogram of color image - contrast enhancement\n",
    "        frame1 = equalizeHistColor(frame)\n",
    "\n",
    "        # Smoothing\n",
    "        gray = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)\n",
    "        blur = cv2.GaussianBlur(gray, (GB_Kernel, GB_Kernel), 0)\n",
    "        # convert to HSV\n",
    "        img_filter = cv2.cvtColor(img_filter, cv2.COLOR_RGB2HSV)\n",
    "        \n",
    "        mask = cv2.inRange(hsv, greenLower, greenUpper)\n",
    "\n",
    "        #img_binary_red = cv2.inRange(img_filter.copy(), THRESHOLD_LOW, THRESHOLD_HIGH)\n",
    "        img_binary_green = cv2.inRange(img_filter.copy(), greenLower, greenUpper)\n",
    "        img_binary_blue = cv2.inRange(img_filter.copy(), blueLower, blueUpper)\n",
    "        mask = cv2.erode(mask, None, iterations=2)\n",
    "        mask = cv2.dilate(mask, None, iterations=2)\n",
    "        \n",
    "        # The bitwise and of the frame and mask is done so that only the blue coloured objects are highlighted and stored in res \n",
    "        res = cv2.bitwise_and(frame,frame, mask= mask) \n",
    "        cv2.imshow('frame',frame) \n",
    "        cv2.imshow('mask',mask) \n",
    "        cv2.imshow('res',res) \n",
    "        \n",
    "        img_contours = img_binary.copy()\n",
    "        contours = cv2.findContours(img_contours, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)[-2]\n",
    "        # Thresholding\n",
    "        # ret, mask = cv2.threshold(blur, threshold1, threshold2, cv2.THRESH_BINARY)\n",
    "        # ret, mask = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "        # ret, mask = cv2.threshold(blur,threshold1, threshold2,cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "#         mask = cv2.adaptiveThreshold(blur,255,cv2.ADAPTIVE_THRESH_MEAN_C,cv2.THRESH_BINARY,11,2)\n",
    "#         mask = cv2.adaptiveThreshold(blur,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C,cv2.THRESH_BINARY,11,2)\n",
    "#         kernel = np.ones((3, 3), np.uint8)\n",
    "#         mask = cv2.erode(mask, kernel, iterations=7)  # morphology erosion\n",
    "#         mask = cv2.dilate(mask, kernel, iterations=5)  # morphology dilation\n",
    "\n",
    "#         mask_inv = cv2.bitwise_not(mask)\n",
    "#         img = cv2.bitwise_and(frame1, frame1, mask=mask_inv)\n",
    "#         img = cv2.addWeighted(frame1, 0.1, img, 0.9, 0)\n",
    "\n",
    "        #Contouring - includes a bounding box - takes from previous gray box\n",
    "        ret, thresh = cv2.threshold(gray, 75, 100, cv2.THRESH_BINARY_INV)   #cv2.CHAIN_APPROX_SIMPLE\n",
    "        img1, contours, hierarchy = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)\n",
    "        if len(contours) != 0:\n",
    "            c = max(contours, key=cv2.contourArea)  # find the largest contour\n",
    "            area = cv2.contourArea(c) # area of the contour\n",
    "            x, y, w, h = cv2.boundingRect(c)  # get bounding box of largest contour\n",
    "            img2=cv2.drawContours(img, c, -1, color, thickness) # draw largest contour\n",
    "            # img2 = cv2.drawContours(frame, contours, -1, color, thickness)  # draw all contours\n",
    "            img3 = cv2.rectangle(img2, (x, y), (x + w, y + h), (0, 0, 255), 2)  # draw red bounding box in img\n",
    "            font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "            cv2.putText(img3, \"label\", (x, h), font, 1,(255,255,255),2)\n",
    "            \n",
    "#             # Print out the location and size (radius) of the largest detected contour\n",
    "#         if center != None:\n",
    "#             print str(center) + \" \" + str(radius) \n",
    "\n",
    "        from this point on working soely with largest contour   \n",
    "        # will output the largest contours (RGB value of each contour mask)\n",
    "        if count <10\n",
    "            baseline = count \n",
    "        elif baseline = average over the last 10 frame\n",
    "            \n",
    "        old_baseline = baseline\n",
    "        average hist\n",
    "        # comparing two hists\n",
    "        double base_base = compareHist( hist_base, hist_base, compare_method )\n",
    "        \n",
    "        if diff = 1 # if different then don't update baseline - otherwise it will washout the difference\n",
    "            \n",
    "        ###########################################################\n",
    "        # Find the largest contour and use it to compute the min enclosing circle\n",
    "        center = None\n",
    "        radius = 0\n",
    "        if len(contours) > 0:\n",
    "            c = max(contours, key=cv2.contourArea)\n",
    "            ((x, y), radius) = cv2.minEnclosingCircle(c)\n",
    "            M = cv2.moments(c)\n",
    "            if M[\"m00\"] > 0:\n",
    "                center = (int(M[\"m10\"] / M[\"m00\"]), int(M[\"m01\"] / M[\"m00\"]))\n",
    "                if radius < MIN_RADIUS:\n",
    "                    center = None\n",
    "        \n",
    "#         for i, cnt in enumerate(contours):\n",
    "\n",
    "#            # compute the bounding box for the contour\n",
    "#            (x, y, w, h) = cv2.boundingRect(cnt)\n",
    "\n",
    "#            # reject contours outside size range\n",
    "#            if w > 250 or w < 30 or h > 250 or h < 30 :\n",
    "#                   continue\n",
    "\n",
    "#            # make sure the box is inside the frame\n",
    "#            if x <= 0 or y <= 0 or x+w >= (WIDTH -1) or y+h >= (HIGHT -1):\n",
    "#                   continue\n",
    "\n",
    "\n",
    "        \n",
    "        #######################################################\n",
    "        colors = ('b', 'g', 'r')\n",
    "\n",
    "        for i, col in enumerate(colors):\n",
    "            hist[i] = cv2.calcHist([imgDEF], [i], None, [256], [1, 256])\n",
    "            plt.plot(hist, color=col)\n",
    "\n",
    "        plt.show()\n",
    "        \n",
    "        # Getting RGB channel sepecifc histograms - this is C+\n",
    "        #####################################################\n",
    "        // Quantize the hue to 30 levels\n",
    "        // and the saturation to 32 levels\n",
    "        int hbins = 30, sbins = 32;\n",
    "        int histSize[] = {hbins, sbins};\n",
    "        // hue varies from 0 to 179, see cvtColor\n",
    "        float hranges[] = { 0, 180 };\n",
    "        // saturation varies from 0 (black-gray-white) to\n",
    "        // 255 (pure spectrum color)\n",
    "        float sranges[] = { 0, 256 };\n",
    "        const float* ranges[] = { hranges, sranges };\n",
    "        MatND hist;\n",
    "        // we compute the histogram from the 0-th and 1-st channels\n",
    "        int channels[] = {0, 1};\n",
    "        \n",
    "        ######################################################\n",
    "        imgDEF = cv2.imread(\"YOUR_IMAGE.jpg\")\n",
    "        imgGray = cv2.cvtColor(imgDEF, cv2.COLOR_BGR2GRAY)    \n",
    "        ghist = cv2.calcHist([imgGray], [0], None, [256], [0,256])\n",
    "\n",
    "        print (\"Mean = {:.1f}, standard deviation = {:.1f}, total = {:.0f}\".format(\n",
    "            np.mean(ghist).item(),\n",
    "            np.std(ghist).item(),\n",
    "            np.sum(ghist).item()\n",
    "        ))\n",
    "\n",
    "        plt.plot(ghist, color='m')\n",
    "        plt.show()\n",
    "        #########################################################\n",
    "        \n",
    "        \n",
    "        # Add frame # to the appropriate structures - Trigger_Start Trigger_Stop are either 1 (green) or 2 (blue), Trigger state_state can also be 0 (neither green nor blue)\n",
    "        # List of [frame + start event trigger] (where the max[index] = corresponds with the last EEG event)\n",
    "        if Trigger_Start = []\n",
    "        # List of [frame + end event trigger]\n",
    "        if Trigger_Stop = []\n",
    "        # List of [frame + trigger state] (0 B + G channels below thresholds, 1 above B channel threshold, 2 above R channel threshold\n",
    "        Tigger_State[count] = \n",
    "        else    \n",
    "            \n",
    "        writer.write(img)  # save the frame into video file\n",
    "        count += 1\n",
    "        if count % 1 == 0:\n",
    "            # cv2.imshow('Original', frame)  # show the original frame\n",
    "            cv2.imshow('New', img) #show the new frame\n",
    "        if cv2.waitKey(1)& 0xFF == ord('q'):\n",
    "            break\n",
    "    else:\n",
    "        print(\"ref != True\")\n",
    "        break\n",
    "    # When everything done, release the capture\n",
    "writer.release()\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "nteract": {
   "version": "0.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
